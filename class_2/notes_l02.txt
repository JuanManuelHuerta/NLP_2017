

Juan M. Huerta
jmhuertany@gmail.com


Class 2: Probability, Entropy, Bayes Theorem, Mutual Information and Part of Speech Tagging

II.0.  Overview of class 2:

      (a) Quick refresher of where we left in class 1                                 [-->>l01s03]
           Challenge Scenario # 1:  Beauty Startup does Market Research

     (b) Focus of second class: Statistical methods (probability), Entropy, POS, Document/Cosine Similarity

II.1  (Prelim: NLTK, download, overview)
      Students are asked to download and install NLTK. 

II.2  Computing P[x|Y]                                                      [l02s01]
      (a) Relation to the Naive Bayes classifier: https://en.wikipedia.org/wiki/Naive_Bayes_classifierz
      (b) Issues: Synonims and plurals get "diluted". Stemming.

II.3 Focus on adjectives (JJ) and then on nouns (NN)                               [l02s02]
     (a) what are the caveats of this analysis?  separation of phrases
     (b) complexity 

II.4  The Posterior probability. Computing P[Y|x] : Bayes Theorem                         
      (a) assume class equally likely


II.5  Entropy of the posterior probability distributions:                         [l02s03]
      (a) Entropy and Mutual Information    

II.6   TF-IDF  Term Frequency Inverse document frequency: per product  
       https://en.wikipedia.org/wiki/Tf%E2%80%93idf
       (a)      Introduce Scenario # 2
          Challenge Scenario # 2:  Identify the product space, Clustering
       (b) Switching to Product based analysis                                    [l02s04]

II.7. Document Similarity (Intro and Overview for next class)
      (a) Similarity of word distributions
      (b) Cosine similarity
      (c) Projections (the problem of synonimity)
      (d) Complexity

       Assignment/Next Class: Cluster products that are similar; produce a visualization




=================


NLTK:
http://www.nltk.org/

Entropy:
https://en.wikipedia.org/wiki/Entropy_(information_theory)
https://en.wikipedia.org/wiki/Mutual_information


Bayes Theorem:
https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/


Computational Complexity:
https://en.wikipedia.org/wiki/Computational_complexity_theory
https://en.wikipedia.org/wiki/Big_O_notation



TF-IDF
http://trimc-nlp.blogspot.com/2013/04/tfidf-with-google-n-grams-and-pos-tags.html


KL Divergence
https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained

